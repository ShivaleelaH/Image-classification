{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGGNet16_SGD_BatchNormalization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhoSnklJRLr2"
      },
      "source": [
        "*   Network     : VGG16\n",
        "*   Optimizer   : SGD\n",
        "*   Regularizer : Batch normalization\n",
        "\n",
        "Ref for code: https://github.com/geifmany/cifar-vgg/blob/master/cifar100vgg.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP_bmdLuRR2E"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras import callbacks\n",
        "\n",
        "from keras.datasets import cifar100\n",
        "from keras.layers.core import Lambda\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puzT6rMORWDR"
      },
      "source": [
        "#Loading cifar100 dataset and dividing into Train, Validation and Test sets\n",
        "\n",
        "(X_train, Y_train), (x_test, y_test_o) = cifar100.load_data()\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train, x_validation, y_train, y_validation = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 100)\n",
        "y_validation = keras.utils.to_categorical(y_validation, 100)\n",
        "y_test = keras.utils.to_categorical(y_test_o, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yGen4exRYu8"
      },
      "source": [
        "#Definition of hyper parameters and other characteristics of the network like the activation functions\n",
        "\n",
        "num_classes = 100\n",
        "weight_decay = 0.0005\n",
        "x_shape = [32,32,3]\n",
        "\n",
        "conv_activation_func = 'elu'\n",
        "dense_activation_func = 'elu'\n",
        "ouput_activation_func = 'softmax'\n",
        "\n",
        "batch_size = 128\n",
        "maxepoches = 100\n",
        "l_rate = 0.01\n",
        "# lr_decay = 1e-6\n",
        "# lr_drop = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIzeh0bIRd9_"
      },
      "source": [
        "#Model definition of VGG16\n",
        "def createModel():\n",
        "  model = Sequential()\n",
        "  weight_decay = 0.0005 #Hyper parameter\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same', input_shape=x_shape))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(Activation(conv_activation_func))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation(conv_activation_func))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(Activation(conv_activation_func))\n",
        "\n",
        "  model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(Activation(conv_activation_func))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  model.add(Conv2D(256, (3, 3), padding='same'))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(Activation(conv_activation_func))\n",
        "\n",
        "  model.add(Conv2D(256, (3, 3), padding='same'))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(Activation(conv_activation_func))\n",
        "\n",
        "  model.add(Conv2D(256, (3, 3), padding='same'))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(Activation(conv_activation_func))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same'))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(Activation(conv_activation_func))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same'))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(Activation(conv_activation_func))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same'))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(Activation(conv_activation_func))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  # model.add(Conv2D(512, (3, 3), padding='same'))\n",
        "  # model.add(BatchNormalization())\n",
        "  # model.add(Activation(conv_activation_func))\n",
        "\n",
        "  # model.add(Conv2D(512, (3, 3), padding='same'))\n",
        "  # model.add(BatchNormalization())\n",
        "  # model.add(Activation(conv_activation_func))\n",
        "\n",
        "  # model.add(Conv2D(512, (3, 3), padding='same'))\n",
        "  # model.add(BatchNormalization())\n",
        "  # model.add(Activation(conv_activation_func))\n",
        "\n",
        "  # model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation(dense_activation_func))\n",
        "\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation(dense_activation_func))\n",
        "\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation(ouput_activation_func))\n",
        "\n",
        "  #Definition of SGD optimizer\n",
        "  # sgd = optimizers.SGD(lr=l_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
        "  sgd = optimizers.SGD(learning_rate=l_rate)\n",
        "\n",
        "  #Compiling the model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eayao3Uv_O3"
      },
      "source": [
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG3MjKFdRe-A"
      },
      "source": [
        "#Creating an instance of the model\n",
        "\n",
        "model = createModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfUbt1SoRhMt"
      },
      "source": [
        "#Defintion of callbacks with EarlyStopping \n",
        "\n",
        "callback = callbacks.EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK1bW7A-Rk-y"
      },
      "source": [
        "#Definition of saving the best weights of the model with ModelCheckpoint\n",
        "\n",
        "checkpoint_path = \"checkpoints/VGGNet16_SGD_BatchNormalization\"\n",
        "checkpoint = callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1, mode='min')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0NRIzpxRlgL",
        "outputId": "a61cc2df-860a-43c0-b965-c5ba1823b333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "#Training the model in the training dataset\n",
        "\n",
        "history = model.fit(datagen.flow(x_train, y_train,\n",
        "                                         batch_size=batch_size),\n",
        "                            steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                            epochs=maxepoches,\n",
        "                            validation_data=(x_validation, y_validation),callbacks=[callback, checkpoint],verbose=2)\n",
        "\n",
        "# history = model.fit(x_train, y_train, steps_per_epoch=x_train.shape[0] // batch_size, epochs=maxepoches, validation_data=(x_validation, y_validation), callbacks=[callback, checkpoint])\n",
        "model.save_weights('VGGNet16_SGD_BatchNormalization.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.80004, saving model to checkpoints/VGGNet16_SGD_BatchNormalization\n",
            "312/312 - 35s - loss: 4.0727 - accuracy: 0.0854 - val_loss: 3.8000 - val_accuracy: 0.1188\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.80004 to 3.42947, saving model to checkpoints/VGGNet16_SGD_BatchNormalization\n",
            "312/312 - 35s - loss: 3.5713 - accuracy: 0.1565 - val_loss: 3.4295 - val_accuracy: 0.1796\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.42947 to 3.27678, saving model to checkpoints/VGGNet16_SGD_BatchNormalization\n",
            "312/312 - 35s - loss: 3.3188 - accuracy: 0.2023 - val_loss: 3.2768 - val_accuracy: 0.2117\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.27678 to 3.00617, saving model to checkpoints/VGGNet16_SGD_BatchNormalization\n",
            "312/312 - 35s - loss: 3.1092 - accuracy: 0.2378 - val_loss: 3.0062 - val_accuracy: 0.2556\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.00617 to 2.85722, saving model to checkpoints/VGGNet16_SGD_BatchNormalization\n",
            "312/312 - 35s - loss: 2.9432 - accuracy: 0.2679 - val_loss: 2.8572 - val_accuracy: 0.2836\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 2.85722\n",
            "312/312 - 35s - loss: 2.7979 - accuracy: 0.2994 - val_loss: 2.9180 - val_accuracy: 0.2773\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 2.85722\n",
            "312/312 - 35s - loss: 2.6642 - accuracy: 0.3257 - val_loss: 3.0544 - val_accuracy: 0.2597\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.85722 to 2.52590, saving model to checkpoints/VGGNet16_SGD_BatchNormalization\n",
            "312/312 - 35s - loss: 2.5506 - accuracy: 0.3495 - val_loss: 2.5259 - val_accuracy: 0.3505\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 2.52590\n",
            "312/312 - 35s - loss: 2.4320 - accuracy: 0.3754 - val_loss: 2.7095 - val_accuracy: 0.3126\n",
            "Epoch 10/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6boU3jvnRvzv"
      },
      "source": [
        "#Plot the Training and Validation accuracy v/s Epochs\n",
        "#history.keys() = ['accuracy', 'loss', 'val_accuracy', 'val_loss']\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzjzJ6n-Rxys"
      },
      "source": [
        "#Plot the Training and Validation loss v/s Epochs\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVvGTVPzR02E"
      },
      "source": [
        "#Testing the model using the best weights of the model\n",
        "\n",
        "model_test = createModel()\n",
        "model_test.load_weights(checkpoint_path)\n",
        "loss, acc = model_test.evaluate(x_test,  y_test, verbose=2)\n",
        "# print(loss, acc)\n",
        "\n",
        "# predict classes for test set\n",
        "predict_y_test = model.predict_classes(x_test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Smt6AqOR2lK"
      },
      "source": [
        "#Precision, recall and accuracy of the model on test data\n",
        " \n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_test_o, predict_y_test)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_test_o, predict_y_test, average='macro')\n",
        "print('Precision: %f' % precision)\n",
        "\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_test_o, predict_y_test, average='macro')\n",
        "print('Recall: %f' % recall)\n",
        "\n",
        "# Confusion matrix\n",
        "confusion_matrix_ = confusion_matrix(y_test_o, predict_y_test)\n",
        "print(confusion_matrix_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvNUWJkjw8dy"
      },
      "source": [
        "import seaborn as sb\n",
        "heat_map = sb.heatmap(confusion_matrix_)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}